# preprocess.py
"""
æ­¤è…³æœ¬è² è²¬ï¼š
1. è¼‰å…¥è¨“ç·´èˆ‡æ¸¬è©¦ JSON æª”æ¡ˆã€‚
2. å°åˆ†é¡æ¬„ä½ï¼ˆCategory, Concept, Subcategoryï¼‰é€²è¡Œ One-hot ç·¨ç¢¼ã€‚
3. ä½¿ç”¨ BERTopic å° Alltags æ¬„ä½åšä¸»é¡Œå»ºæ¨¡ä¸¦åŠ å…¥ topic å‘é‡ã€‚
4. å° One-hot ç·¨ç¢¼çµæœåš PCA é™ç¶­ï¼ˆç¬¦åˆè«–æ–‡åšæ³•ï¼‰ã€‚
5. åŠ å…¥ GraphSAGE hashtag åµŒå…¥ï¼ˆå…±ç¾åœ–ç”¢å‡ºï¼‰
6. åˆä½µè¨“ç·´è³‡æ–™èˆ‡å°æ‡‰æ¨™ç±¤ã€‚
7. å°‡è™•ç†å¾Œè³‡æ–™å„²å­˜æˆ CSVï¼Œä¸¦å„²å­˜ Encoder èˆ‡ PCA æ¨¡å‹ã€‚
"""

import pandas as pd
import json
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
import pickle
import os
from bertopic import BERTopic
import numpy as np
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è¼‰å…¥è¨“ç·´èˆ‡æ¸¬è©¦ JSONï¼ˆæ ¼å¼ç‚º JSON é™£åˆ—ï¼‰
with open('./data/train_data.json') as f:
    train_data = json.load(f)
with open('./data/test_data.json') as f:
    test_data = json.load(f)

train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)
train_len = len(train_df)

# ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
cat_cols = ['Category', 'Concept', 'Subcategory']
for col in cat_cols:
    if col not in train_df.columns:
        train_df[col] = ""
    if col not in test_df.columns:
        test_df[col] = ""

# åˆä½µè³‡æ–™ä»¥ä¾¿åš fit
all_df = pd.concat([train_df, test_df], axis=0)

# One-hot encode é¡åˆ¥æ¬„
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded = encoder.fit_transform(all_df[cat_cols])
encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_cols))

# === ä½¿ç”¨ BERTopic å° Alltags åšä¸»é¡Œå»ºæ¨¡ ===
print("ğŸš€ å»ºç«‹ BERTopic æ¨¡å‹...")
docs = all_df['Alltags'].astype(str).fillna("None").tolist()
topic_model = BERTopic(language="multilingual", calculate_probabilities=True)
topics, probs = topic_model.fit_transform(docs)
topic_df = pd.DataFrame(probs, columns=[f"Topic_{i}" for i in range(probs.shape[1])])
print(f"âœ… BERTopic ä¸»é¡Œç¶­åº¦ï¼š{topic_df.shape[1]}")

# === å° One-hot encoded social features åš PCA é™ç¶­ ===
print("ğŸ“‰ åŸ·è¡Œ PCA é™ç¶­...")
social_cols = encoded_df.columns.tolist()
pca = PCA(n_components=6)  # é™åˆ° 6 ç¶­
social_pca = pca.fit_transform(encoded_df[social_cols])
social_df = pd.DataFrame(social_pca, columns=[f"social_pca_{i}" for i in range(social_pca.shape[1])])

# === åŠ å…¥ GraphSAGE hashtag åµŒå…¥ï¼ˆå¹³å‡æ‰€æœ‰æ¨™ç±¤ç¯€é»ï¼‰ ===
print("ğŸŒ è¼‰å…¥ GraphSAGE hashtag åµŒå…¥...")
with open('./experiments/checkpoints/graph_emb_dict.pkl', 'rb') as f:
    graph_emb_dict = pickle.load(f)
emb_dim = next(iter(graph_emb_dict.values())).shape[0]

def get_graph_emb_vector(tag_str):
    tags = tag_str.strip().split()
    vecs = [graph_emb_dict[t] for t in tags if t in graph_emb_dict]
    if vecs:
        return np.mean(vecs, axis=0)
    else:
        return np.zeros(emb_dim)

graph_emb_arr = all_df['Alltags'].astype(str).apply(get_graph_emb_vector)
graph_emb_df = pd.DataFrame(graph_emb_arr.tolist(), columns=[f"graph_emb_{i}" for i in range(emb_dim)])

# åˆä½µæ‰€æœ‰ç‰¹å¾µ
all_df = all_df.reset_index(drop=True)
all_df = pd.concat([all_df, topic_df, social_df, graph_emb_df], axis=1)
print(f"âœ… all-dfç¶­åº¦ï¼ˆå« topic+social_pca+graph_embï¼‰ï¼š{all_df.shape}")

# åˆ‡å› train / test
train_df = all_df.iloc[:train_len].copy()
test_df = all_df.iloc[train_len:].copy()

# åˆä½µ label
label_df = pd.read_csv('./data/train_label.csv')
train_df = train_df.merge(label_df, on='Pid', how='left')

# å„²å­˜ CSV
train_df.to_csv('./data/train.csv', index=False)
test_df.to_csv('./data/test.csv', index=False)

# å„²å­˜ Encoder èˆ‡ PCA
os.makedirs('./experiments/checkpoints', exist_ok=True)
with open('./experiments/checkpoints/encoder.pkl', 'wb') as f:
    pickle.dump(encoder, f)
with open('./experiments/checkpoints/pca.pkl', 'wb') as f:
    pickle.dump(pca, f)

# å„²å­˜ BERTopic æ¨¡å‹ï¼ˆå¯é¸ï¼‰
topic_model.save("./experiments/checkpoints/bertopic_model")