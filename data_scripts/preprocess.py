import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_NO_TF"] = "1"

from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from bertopic import BERTopic
from tqdm import tqdm
import pandas as pd
import json
import numpy as np
import pickle
import glob

# åƒæ•¸
cat_cols = ['Category', 'Concept', 'Subcategory']
emb_path = './experiments/checkpoints/graph_emb_dict.pkl'
pca_dim = 6
image_root = './data/train'  # åœ–ç‰‡è³‡æ–™æ ¹ç›®éŒ„

# è¼‰å…¥ JSON
with open('./data/train_data.json') as f:
    train_data = json.load(f)
with open('./data/test_data.json') as f:
    test_data = json.load(f)

train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)
train_len = len(train_df)

# è£œé½Šæ¬„ä½
for col in cat_cols:
    train_df[col] = train_df.get(col, "")
    test_df[col] = test_df.get(col, "")

# åˆä½µè³‡æ–™
all_df = pd.concat([train_df, test_df], axis=0)


# æ­¥é©Ÿ 1ï¼šå…ˆè™•ç†ç¼ºæ¼æ¨™è¨˜æ¬„ä½
all_df['is_title_missing'] = all_df['Title'].fillna('').astype(str).str.strip().eq('').astype(int)
# æ­¥é©Ÿ 2ï¼šå†æŠŠç©ºå­—ä¸²æ”¹ç‚º 'None'
all_df['Title'] = all_df['Title'].fillna('').astype(str).apply(lambda x: 'None' if x.strip() == '' else x)

# æ™‚é–“ç›¸é—œ
all_df['post_dt'] = pd.to_datetime(all_df['Postdate'], unit='s')
# å¹¾é»ç™¼æ–‡ï¼ˆ0~23ï¼‰
all_df['post_hour'] = all_df['post_dt'].dt.hour
# ç¦®æ‹œå¹¾ï¼ˆ0=Mon, 6=Sunï¼‰
all_df['post_weekday'] = all_df['post_dt'].dt.weekday
# æ˜¯å¦å¤œé–“ç™¼æ–‡ï¼ˆ0~6 â†’ æ—©ä¸Šï¼Œ 18~23 â†’ æ™šä¸Šï¼Œå…¶ä»–ç‚ºç™½å¤©ï¼‰
def classify_period(h):
    if h < 6:
        return 'midnight'
    elif h < 12:
        return 'morning'
    elif h < 18:
        return 'afternoon'
    else:
        return 'evening'
all_df['post_period'] = all_df['post_hour'].apply(classify_period)

# One-hot encode åˆ†é¡æ¬„  
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded = encoder.fit_transform(all_df[cat_cols])
encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_cols), index=all_df.index)

# åŠ å…¥å…¶ä»– social ç‰¹å¾µ
encoded_df['is_title_missing'] = all_df['is_title_missing'].values
# æ•´æ•¸é¡æ™‚é–“ç‰¹å¾µ
encoded_df['post_hour'] = all_df['post_hour']
encoded_df['post_weekday'] = all_df['post_weekday']
# å°‡ post_period åš one-hot ç·¨ç¢¼ï¼ˆmorning / afternoon / evening / midnightï¼‰
period_dummies = pd.get_dummies(all_df['post_period'], prefix='period')
encoded_df = pd.concat([encoded_df, period_dummies], axis=1)

# æª¢æŸ¥ hashtag ç¼ºæ¼æ•¸
empty_tag_count = all_df['Alltags'].astype(str).str.strip().eq('').sum()
print(f"ğŸ” æ²’æœ‰ hashtag çš„è²¼æ–‡æ•¸é‡ï¼š{empty_tag_count}")

# BERTopic
print("ğŸš€ å»ºç«‹ BERTopic æ¨¡å‹...")
docs = all_df['Alltags'].astype(str).fillna("None").tolist()
topic_model = BERTopic(language="multilingual", min_topic_size=10, calculate_probabilities=True)
topics, probs = topic_model.fit_transform(docs)
topic_df = pd.DataFrame(probs, columns=[f"Topic_{i}" for i in range(probs.shape[1])])
topic_df = topic_df.fillna(0)
print(f"âœ… BERTopic ä¸»é¡Œç¶­åº¦ï¼š{topic_df.shape[1]}")

# PCA é™ç¶­ç¤¾ç¾¤ç‰¹å¾µ
print("ğŸ“‰ åŸ·è¡Œ PCA é™ç¶­...")
pca = PCA(n_components=pca_dim)
social_pca = pca.fit_transform(encoded_df)
social_df = pd.DataFrame(social_pca, columns=[f"social_pca_{i}" for i in range(pca_dim)])

# GraphSAGE hashtag åµŒå…¥
print("ğŸŒ è¼‰å…¥ GraphSAGE hashtag åµŒå…¥...")
with open(emb_path, 'rb') as f:
    graph_emb_dict = pickle.load(f)
emb_dim = next(iter(graph_emb_dict.values())).shape[0]

def get_graph_emb_vector(tag_str):
    if not isinstance(tag_str, str):
        return np.zeros(emb_dim)
    tags = tag_str.strip().split()
    vecs = [graph_emb_dict[t] for t in tags if t in graph_emb_dict]
    return np.mean(vecs, axis=0) if vecs else np.zeros(emb_dim)

print("ğŸ” è¨ˆç®— hashtag å¹³å‡åµŒå…¥...")
tqdm.pandas()
graph_emb_arr = all_df['Alltags'].astype(str).progress_apply(get_graph_emb_vector)
graph_emb_df = pd.DataFrame(graph_emb_arr.tolist(), columns=[f"graph_emb_{i}" for i in range(emb_dim)])

no_embed_count = sum((not any(t in graph_emb_dict for t in str(tags).strip().split())) for tags in all_df['Alltags'])
print(f"âš ï¸ æ²’æœ‰æœ‰æ•ˆ hashtag åµŒå…¥çš„æ¨£æœ¬æ•¸ï¼š{no_embed_count}")

# åˆä½µæ‰€æœ‰ç‰¹å¾µ
all_df = all_df.reset_index(drop=True)
all_df = pd.concat([all_df, topic_df, social_df, graph_emb_df], axis=1)
print(f"âœ… åˆä½µå®Œæˆï¼Œall_df ç¶­åº¦ï¼š{all_df.shape}")

# åˆ‡å› train / test
train_df = all_df.iloc[:train_len].copy()
test_df = all_df.iloc[train_len:].copy()

# åˆä½µ label
label_df = pd.read_csv('./data/train_label.csv')
train_df = train_df.merge(label_df, on='Pid', how='left')

# å„²å­˜è³‡æ–™èˆ‡æ¨¡å‹
train_df.to_csv('./data/train.csv', index=False)
test_df.to_csv('./data/test.csv', index=False)

os.makedirs('./experiments/checkpoints', exist_ok=True)
with open('./experiments/checkpoints/encoder.pkl', 'wb') as f:
    pickle.dump(encoder, f)
with open('./experiments/checkpoints/pca.pkl', 'wb') as f:
    pickle.dump(pca, f)

topic_model.save("./experiments/checkpoints/bertopic_model")
print("âœ… è³‡æ–™é è™•ç†å®Œæˆï¼Œå·²å„²å­˜è‡³ CSV èˆ‡æ¨¡å‹æª”æ¡ˆ")
